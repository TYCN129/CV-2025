{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/jessicali9530/celeba-dataset?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.00/1.33G [00:00<?, ?B/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import kagglehub\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "LATENT_DIM = 128\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 50\n",
    "\n",
    "# Download dataset using kagglehub\n",
    "path = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n",
    "data_dir = Path(path) / \"img_align_celeba\"\n",
    "\n",
    "# Dataset transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to 128x128\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load CelebA dataset from KaggleHub\n",
    "dataset = ImageFolder(root=str(data_dir.parent), transform=transform)\n",
    "\n",
    "# Select random 80k images from first 162770 images\n",
    "torch.manual_seed(42)\n",
    "indices = torch.randperm(162770)[:80000]\n",
    "train_dataset = Subset(dataset, indices.tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256 * 8 * 8, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 8 * 8, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, 256 * 8 * 8)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x).view(x.size(0), -1)\n",
    "        mu, logvar = self.fc_mu(x), self.fc_logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.fc_decode(z).view(z.size(0), 256, 8, 8)\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# Train the model\n",
    "vae = VAE(LATENT_DIM).to(device)\n",
    "\n",
    "# Load the model from vae_model_celeb_50.pth\n",
    "vae.load_state_dict(torch.load(\"vae_model_celeba_50_convention_gpt.pth\"))\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(vae.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     vae.train()\n",
    "#     train_loss = 0\n",
    "#     for imgs, _ in train_loader:\n",
    "#         imgs = imgs.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         recon_imgs, mu, logvar = vae(imgs)\n",
    "#         loss = loss_function(recon_imgs, imgs, mu, logvar)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss/len(dataset):.4f}\")\n",
    "\n",
    "# print(\"Training complete!\")\n",
    "\n",
    "# Display some reconstructions\n",
    "def show_reconstructions(loader, title):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        imgs, _ = next(iter(loader))\n",
    "        imgs = imgs.to(device)\n",
    "        recon_imgs, _, _ = vae(imgs)\n",
    "        imgs, recon_imgs = imgs.cpu(), recon_imgs.cpu()\n",
    "        fig, axes = plt.subplots(2, 8, figsize=(12, 4))\n",
    "        for i in range(8):\n",
    "            axes[0, i].imshow(imgs[i].permute(1, 2, 0))\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].imshow(recon_imgs[i].permute(1, 2, 0))\n",
    "            axes[1, i].axis('off')\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "\n",
    "show_reconstructions(train_loader, \"Train Data Reconstructions\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
